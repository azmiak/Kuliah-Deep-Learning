{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Chapter 11: Training Deep Neural Networks\n",
        "\n",
        "**Tujuan:** Menangani *vanishing/exploding gradients*, inisialisasi bobot, fungsi aktivasi, *Batch Normalization*, *gradient clipping*, optimizers, dan *transfer learning*.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Vanishing & Exploding Gradients\n",
        "\n",
        "* Saat *backpropagation*, gradien bisa:\n",
        "\n",
        "  * **Vanishing**: mendekati nol → pelatihan lambat atau berhenti\n",
        "  * **Exploding**: terlalu besar → ketidakstabilan\n",
        "* Umumnya terjadi di jaringan yang sangat dalam.\n",
        "* **Solusi:**\n",
        "\n",
        "  * Inisialisasi bobot yang tepat (Glorot, He)\n",
        "  * Aktivasi non-saturating (ReLU, LeakyReLU)\n",
        "  * *Batch Normalization*\n",
        "  * *Gradient Clipping*\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Inisialisasi Bobot\n",
        "\n",
        "* **Glorot/Xavier Initialization**:\n",
        "\n",
        "  $$\n",
        "  W \\sim \\mathcal{U} \\left[ -\\sqrt{\\frac{6}{n_{in} + n_{out}}}, \\sqrt{\\frac{6}{n_{in} + n_{out}}} \\right]\n",
        "  $$\n",
        "\n",
        "* **He Initialization** (cocok untuk ReLU):\n",
        "\n",
        "  $$\n",
        "  W \\sim \\mathcal{N} \\left( 0, \\sqrt{\\frac{2}{n_{in}}} \\right)\n",
        "  $$\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Aktivasi Non‑Saturating\n",
        "\n",
        "* **ReLU**:\n",
        "\n",
        "  $\n",
        "  \\text{ReLU}(z) = \\max(0, z)\n",
        "  $\n",
        "\n",
        "* Varian lain:\n",
        "\n",
        "  * **LeakyReLU**: mencegah neuron “mati”\n",
        "  * **ELU**, **SELU**: self-normalizing properties (SELU butuh inisialisasi dan dropout khusus)\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Batch Normalization\n",
        "\n",
        "* Menstabilkan pelatihan dengan *menormalisasi* output layer:\n",
        "\n",
        "  * Normalisasi: $\\hat{x} = \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}$\n",
        "  * Skala dan geser: $y = \\gamma \\hat{x} + \\beta$ (dilatih)\n",
        "* Dapat mempercepat dan menstabilkan pelatihan.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Gradient Clipping\n",
        "\n",
        "* Mencegah *exploding gradient* dengan membatasi ukuran gradien:\n",
        "\n",
        "  * **Clip by value**: limit gradien ke rentang tetap\n",
        "  * **Clip by norm**: skalakan jika norm melebihi threshold\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Optimizers\n",
        "\n",
        "| Optimizer    | Karakteristik                                  |\n",
        "| ------------ | ---------------------------------------------- |\n",
        "| **SGD**      | Dasar, butuh learning rate yang hati‑hati      |\n",
        "| **Momentum** | Tambahkan inersia ke arah gradien              |\n",
        "| **Nesterov** | Lookahead momentum (lebih responsif)           |\n",
        "| **AdaGrad**  | Adaptif, cocok untuk data sparse               |\n",
        "| **RMSprop**  | Mirip AdaGrad tapi lebih stabil jangka panjang |\n",
        "| **Adam**     | Gabungan Momentum + RMSprop (paling umum)      |\n",
        "| **Nadam**    | Adam + Nesterov momentum                       |\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "QKqxklGCLZsW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Transfer Learning\n",
        "\n",
        "- Gunakan model pretrained (misal MobileNet) → _fine‑tune_ layer atas untuk dataset baru  \n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "AAgSn2B2MGK1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "akFaeSB5Il2h",
        "outputId": "a876ebd6-aae5-496f-d6a9-647bc8956950"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model1 (no BN)...\n",
            "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - accuracy: 0.5803 - loss: 1.0949 - val_accuracy: 0.8350 - val_loss: 0.4688\n",
            "\n",
            "Training model2 (BN + He + clip_norm=1)...\n",
            "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 30ms/step - accuracy: 0.6350 - loss: 1.1070 - val_accuracy: 0.8178 - val_loss: 0.4966\n",
            "\n",
            "Evaluate model1:\n",
            "[0.4947534203529358, 0.8256000280380249]\n",
            "Evaluate model2:\n",
            "[0.5355045199394226, 0.8062999844551086]\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, initializers, optimizers, callbacks\n",
        "\n",
        "# Setup: buat dataset sintetik untuk classification\n",
        "(X_train, y_train), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
        "X_train = X_train.reshape(-1,28,28,1).astype(\"float32\")/255\n",
        "X_test  = X_test .reshape(-1,28,28,1).astype(\"float32\")/255\n",
        "\n",
        "# Create a simple deep MLP to illustrate vanishing/exploding\n",
        "def build_deep_model(init, activation, use_bn=False, clip_norm=None):\n",
        "    inp = keras.Input(shape=(28,28,1))\n",
        "    x = layers.Flatten()(inp)\n",
        "    for _ in range(10):\n",
        "        x = layers.Dense(128,\n",
        "                         activation=activation,\n",
        "                         kernel_initializer=init)(x)\n",
        "        if use_bn:\n",
        "            x = layers.BatchNormalization()(x)\n",
        "    out = layers.Dense(10, activation=\"softmax\")(x)\n",
        "    opt = optimizers.Adam(clipnorm=clip_norm)\n",
        "    model = keras.Model(inp, out)\n",
        "    model.compile(optimizer=opt,\n",
        "                  loss=\"sparse_categorical_crossentropy\",\n",
        "                  metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "# 1) Tanpa BN, default init → mungkin vanishing\n",
        "model1 = build_deep_model(\"glorot_uniform\", \"relu\", use_bn=False, clip_norm=None)\n",
        "# 2) Dengan BatchNorm + He init + gradient clipping\n",
        "model2 = build_deep_model(initializers.HeNormal(), \"relu\", use_bn=True, clip_norm=1.0)\n",
        "\n",
        "# Train singkat tiap model (1 epoch untuk demo)\n",
        "print(\"Training model1 (no BN)...\")\n",
        "model1.fit(X_train, y_train, epochs=1, batch_size=256, validation_split=0.1)\n",
        "print(\"\\nTraining model2 (BN + He + clip_norm=1)...\")\n",
        "model2.fit(X_train, y_train, epochs=1, batch_size=256, validation_split=0.1)\n",
        "\n",
        "# Evaluate\n",
        "print(\"\\nEvaluate model1:\")\n",
        "print(model1.evaluate(X_test, y_test, verbose=0))\n",
        "print(\"Evaluate model2:\")\n",
        "print(model2.evaluate(X_test, y_test, verbose=0))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Transfer Learning dengan MobileNetV2"
      ],
      "metadata": {
        "id": "x1V_vUYOMU_H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import mixed_precision\n",
        "mixed_precision.set_global_policy('mixed_float16')\n",
        "\n",
        "# Demo: resize & subset CIFAR10\n",
        "(x_c, y_c), _ = keras.datasets.cifar10.load_data()\n",
        "x_c = tf.image.resize(x_c, (96,96))[:2500] / 255.0\n",
        "y_c = y_c[:2500]\n",
        "\n",
        "# Load MobileNetV2 pretrained (tanpa top)\n",
        "base = keras.applications.MobileNetV2(\n",
        "    input_shape=(96,96,3),\n",
        "    include_top=False,\n",
        "    weights=\"imagenet\",\n",
        "    alpha=0.35\n",
        ")\n",
        "base.trainable = False  # freeze base\n",
        "\n",
        "# Tambah classifier baru\n",
        "inputs = keras.Input(shape=(96,96,3))\n",
        "x = base(inputs, training=False)\n",
        "x = layers.GlobalAveragePooling2D()(x)\n",
        "x = layers.Dropout(0.2)(x)\n",
        "outputs = layers.Dense(10, activation=\"softmax\")(x)\n",
        "tl_model = keras.Model(inputs, outputs)\n",
        "\n",
        "tl_model.compile(optimizer=\"adam\",\n",
        "                 loss=\"sparse_categorical_crossentropy\",\n",
        "                 metrics=[\"accuracy\"])\n",
        "\n",
        "# Latih\n",
        "tl_model.fit(x_c, y_c, epochs=3, batch_size=16, validation_split=0.1)"
      ],
      "metadata": {
        "id": "KmtK0LA_MYcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ringkasan Chapter 11\n",
        "- Vanishing/exploding diatasi dengan inisialisasi, ReLU, BatchNorm, clipping\n",
        "\n",
        "- Adam sering jadi pilihan default\n",
        "\n",
        "- Transfer Learning percepat pelatihan dan tingkatkan performa"
      ],
      "metadata": {
        "id": "GWtL-jzvMdpV"
      }
    }
  ]
}