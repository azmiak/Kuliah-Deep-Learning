{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 13: Loading and Preprocessing Data with TensorFlow\n",
        "\n",
        "**Tujuan:** Menguasai `tf.data` API untuk memuat & memroses data secara efisien, termasuk:\n",
        "- Membaca dari array, CSV, dan TFRecord  \n",
        "- Transformasi (map, shuffle, batch, prefetch)  \n",
        "- Preprocessing layers (`Normalization`, `CategoryEncoding`, `StringLookup`)  \n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "j6_eJDsQbBjz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. The `tf.data.Dataset` API\n",
        "\n",
        "Dengan `tf.data`, Anda dapat membangun pipeline data yang:\n",
        "1. **Memuat** data (`from_tensor_slices`, `from_csv`, `TFRecordDataset`)  \n",
        "2. **Transform**: `.map()`, `.filter()`  \n",
        "3. **Shuffle** & **Batch**  \n",
        "4. **Prefetch** untuk overlap I/O & komputasi\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "qGGyjP5SbMcI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YvORblHjbABC",
        "outputId": "2f061333-3948-482a-84c2-e8579f6f3792"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32, 5) (32,)\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import data\n",
        "\n",
        "# Contoh: dataset sederhana dari array NumPy\n",
        "import numpy as np\n",
        "\n",
        "X = np.random.rand(1000, 5).astype(\"float32\")\n",
        "y = (np.sum(X, axis=1) > 2.5).astype(\"int32\")\n",
        "\n",
        "ds = tf.data.Dataset.from_tensor_slices((X, y))\n",
        "ds = ds.shuffle(buffer_size=1000).batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Iterasi singkat\n",
        "for x_batch, y_batch in ds.take(1):\n",
        "    print(x_batch.shape, y_batch.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Membaca CSV dengan `tf.data`"
      ],
      "metadata": {
        "id": "0nH2ksAxbTuE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Misal kita punya file CSV 'data.csv' dengan header:\n",
        "# feature1,feature2,...,label\n",
        "# 0.1,0.2,...,0\n",
        "\n",
        "# Definisikan parsing function\n",
        "def parse_csv(line):\n",
        "    # DefaultTextLineDataset mengembalikan satu string per baris\n",
        "    defaults = [0.0] * 5 + [0]    # 5 fitur float + 1 label int\n",
        "    fields = tf.io.decode_csv(line, record_defaults=defaults)\n",
        "    features = tf.stack(fields[:-1], axis=0)\n",
        "    label    = fields[-1]\n",
        "    return features, label\n",
        "\n",
        "# Bangun dataset\n",
        "csv_ds = tf.data.TextLineDataset(\"data.csv\") \\\n",
        "              .skip(1) \\\n",
        "              .map(parse_csv) \\\n",
        "              .shuffle(1000) \\\n",
        "              .batch(32) \\\n",
        "              .prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "Q2LUwtYqbaep"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. TFRecord: Format Binary Efisien"
      ],
      "metadata": {
        "id": "-RAFjwmybrtY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Menulis TFRecord"
      ],
      "metadata": {
        "id": "AW_Nt8_5buNF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fungsi bantu untuk serialisasi\n",
        "def serialize_example(features, label):\n",
        "    feature = {\n",
        "        \"features\": tf.train.Feature(float_list=tf.train.FloatList(value=features)),\n",
        "        \"label\":    tf.train.Feature(int64_list=tf.train.Int64List(value=[label]))\n",
        "    }\n",
        "    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
        "    return example_proto.SerializeToString()\n",
        "\n",
        "# Tulis ke file\n",
        "with tf.io.TFRecordWriter(\"data.tfrecord\") as writer:\n",
        "    for f, l in zip(X, y):\n",
        "        writer.write(serialize_example(f.tolist(), int(l)))"
      ],
      "metadata": {
        "id": "6qCMmhbUbyB3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Membaca TFRecord"
      ],
      "metadata": {
        "id": "TdixV5mbbycd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_ds = tf.data.TFRecordDataset(\"data.tfrecord\")\n",
        "\n",
        "# Definisikan parsing spec\n",
        "feature_spec = {\n",
        "    \"features\": tf.io.FixedLenFeature([5], tf.float32),\n",
        "    \"label\":    tf.io.FixedLenFeature([],   tf.int64),\n",
        "}\n",
        "\n",
        "def parse_tfrecord(example_proto):\n",
        "    parsed = tf.io.parse_single_example(example_proto, feature_spec)\n",
        "    return parsed[\"features\"], parsed[\"label\"]\n",
        "\n",
        "tfrecord_ds = raw_ds.map(parse_tfrecord) \\\n",
        "                    .shuffle(1000) \\\n",
        "                    .batch(32) \\\n",
        "                    .prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "QRlPavgbbkuB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Preprocessing Layers di Keras\n",
        "Keras menyediakan layer preprocessing yang bisa disertakan dalam model:\n",
        "\n",
        "1. Normalization → normalisasi `mean=0, std=1`\n",
        "\n",
        "2. StringLookup & CategoryEncoding → mapping string → integer → one‑hot\n",
        "\n",
        "3. Discretization, Hashing, TextVectorization"
      ],
      "metadata": {
        "id": "v90kgXV9b7f7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "# 4.1 Contoh Normalization\n",
        "num_data = np.random.rand(1000,3).astype(\"float32\")\n",
        "norm_layer = layers.Normalization(axis=-1)\n",
        "norm_layer.adapt(num_data)  # hitung mean & var\n",
        "\n",
        "print(\"Mean:\", norm_layer.mean.numpy())\n",
        "print(\"Transformed:\", norm_layer(num_data[:2]))\n",
        "\n",
        "# 4.2 Contoh Category Encoding\n",
        "raw_cat = np.array([[\"apple\"], [\"banana\"], [\"orange\"], [\"banana\"]])\n",
        "str_lookup = layers.StringLookup(output_mode=\"one_hot\")\n",
        "str_lookup.adapt(raw_cat)\n",
        "print(\"Encoded:\", str_lookup(raw_cat))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uw5TEne_cMYb",
        "outputId": "2ee3fe3b-aba0-4611-f9cd-ebc51faa6cab"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean: [[0.47925436 0.5087743  0.504174  ]]\n",
            "Transformed: tf.Tensor(\n",
            "[[ 1.3886446  -1.0806028   1.0751902 ]\n",
            " [-0.24987271  1.6817173   0.10524415]], shape=(2, 3), dtype=float32)\n",
            "Encoded: tf.Tensor(\n",
            "[[0 0 0 1]\n",
            " [0 1 0 0]\n",
            " [0 0 1 0]\n",
            " [0 1 0 0]], shape=(4, 4), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Pipeline Lengkap dalam Model\n",
        "Gabungkan `tf.data` + preprocessing layer dalam `tf.keras.Sequential`:"
      ],
      "metadata": {
        "id": "13VkJkelcNMw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset dummy\n",
        "(ds_X, ds_y), _ = tf.keras.datasets.boston_housing.load_data()\n",
        "ds = tf.data.Dataset.from_tensor_slices((ds_X, ds_y)) \\\n",
        "                   .shuffle(512) \\\n",
        "                   .batch(32) \\\n",
        "                   .prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Model dengan preprocessing\n",
        "model = tf.keras.Sequential([\n",
        "    layers.Normalization(input_shape=(ds_X.shape[1],)),\n",
        "    layers.Dense(64, activation=\"relu\"),\n",
        "    layers.Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
        "model.fit(ds, epochs=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QiZtIiWCca26",
        "outputId": "1f204d2c-8951-40b9-c331-afaf8f6ac381"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/boston_housing.npz\n",
            "\u001b[1m57026/57026\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/preprocessing/tf_data_layer.py:19: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - loss: 2067.8362 - mae: 39.7209\n",
            "Epoch 2/5\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 581.4373 - mae: 17.9957\n",
            "Epoch 3/5\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 199.2198 - mae: 10.8742\n",
            "Epoch 4/5\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 117.4487 - mae: 8.3402\n",
            "Epoch 5/5\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 114.2525 - mae: 8.0627 \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7dcefcb380d0>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ringkasan Chapter 13\n",
        "1. `tf.data.Dataset` untuk pipeline data efisien (map, shuffle, batch, prefetch).\n",
        "\n",
        "2. Bisa membaca array, CSV, TFRecord (format binary).\n",
        "\n",
        "3. Keras punya preprocessing layers untuk normalisasi & encoding.\n",
        "\n",
        "4. Gabungkan pipeline data & model dalam satu graph untuk performa maksimal."
      ],
      "metadata": {
        "id": "qDHIFQkVccXE"
      }
    }
  ]
}