{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsH6z1PRYDaT"
      },
      "source": [
        "# Chapter 18: Reinforcement Learning\n",
        "\n",
        "**Tujuan:** Memahami konsep dasar Reinforcement Learning (RL), Markov Decision Process, Q-Learning (tabular), Policy Gradient (REINFORCE), dan Deep Q-Network (DQN) dengan contoh praktis.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. RL Fundamentals\n",
        "\n",
        "* **Agent** berinteraksi dengan **Environment**\n",
        "\n",
        "* Pada tiap timestep $t$:\n",
        "\n",
        "  * Agent mengamati state $s_t$\n",
        "  * Memilih aksi $a_t$\n",
        "  * Mendapat reward $r_{t+1}$\n",
        "  * Pindah ke state berikutnya $s_{t+1}$\n",
        "\n",
        "* Tujuan: memaksimalkan *expected discounted return*:\n",
        "\n",
        "  $\n",
        "  G_t = \\sum_{k=0}^{\\infty} \\gamma^k r_{t+k+1}\n",
        "  $\n",
        "\n",
        "  dengan $\\gamma \\in [0,1)$ sebagai discount factor.\n",
        "\n",
        "---\n",
        "\n",
        "### 1.1 Markov Decision Process (MDP)\n",
        "\n",
        "* Didefinisikan oleh tuple $(\\mathcal{S}, \\mathcal{A}, P, R, \\gamma)$:\n",
        "\n",
        "  * $\\mathcal{S}$: himpunan state\n",
        "  * $\\mathcal{A}$: himpunan aksi\n",
        "  * $P(s'|s,a)$: probabilitas transisi\n",
        "  * $R(s,a)$: reward yang diharapkan\n",
        "  * $\\gamma$: discount factor\n",
        "\n",
        "* **Policy** $\\pi(a|s)$: probabilitas memilih aksi $a$ pada state $s$\n",
        "\n",
        "* **Value function** (nilai ekspektasi suatu state):\n",
        "\n",
        "  $\n",
        "  V^\\pi(s) = \\mathbb{E}_\\pi \\left[ \\sum_{k=0}^{\\infty} \\gamma^k r_{t+k+1} \\,\\middle|\\, s_t = s \\right]\n",
        "  $\n",
        "\n",
        "* **Q-function** (nilai ekspektasi untuk pasangan state dan aksi):\n",
        "\n",
        "  $\n",
        "  Q^\\pi(s,a) = \\mathbb{E}_\\pi \\left[ \\sum_{k=0}^{\\infty} \\gamma^k r_{t+k+1} \\,\\middle|\\, s_t = s, a_t = a \\right]\n",
        "  $\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbQ_Ht1RZvm_"
      },
      "source": [
        "## 2. Tabular Q-Learning (Off-Policy)\n",
        "\n",
        "* Merupakan metode off-policy TD control\n",
        "\n",
        "* **Aturan update:**\n",
        "\n",
        "  $\n",
        "  Q(s,a) \\leftarrow Q(s,a) + \\alpha \\left[ r + \\gamma \\max_{a'} Q(s', a') - Q(s,a) \\right]\n",
        "  $\n",
        "\n",
        "  * $\\alpha$: learning rate\n",
        "  * $\\gamma$: discount factor\n",
        "  * $r$: reward langsung\n",
        "  * $s'$: state berikutnya\n",
        "  * $a'$: aksi optimal pada $s'$\n",
        "\n",
        "* **Eksplorasi vs Eksploitasi**:\n",
        "\n",
        "  * Gunakan strategi **ε-greedy**:\n",
        "\n",
        "    * Pilih aksi acak dengan probabilitas $\\varepsilon$\n",
        "    * Pilih aksi terbaik (dari Q-table) dengan probabilitas $1 - \\varepsilon$\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCRHA2SzW-i-",
        "outputId": "cf810009-2791-4760-d58a-f813cc8e3d5c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.11/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode  1000, ε=0.010, eval avg reward=1.00\n",
            "Episode  2000, ε=0.010, eval avg reward=1.00\n",
            "Episode  3000, ε=0.010, eval avg reward=1.00\n",
            "Episode  4000, ε=0.010, eval avg reward=1.00\n",
            "Episode  5000, ε=0.010, eval avg reward=1.00\n",
            "Episode  6000, ε=0.010, eval avg reward=1.00\n",
            "Episode  7000, ε=0.010, eval avg reward=1.00\n",
            "Episode  8000, ε=0.010, eval avg reward=1.00\n",
            "Episode  9000, ε=0.010, eval avg reward=1.00\n",
            "Episode 10000, ε=0.010, eval avg reward=1.00\n",
            "Final average reward over 200 eval episodes: 1.0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "if not hasattr(np, \"bool8\"):\n",
        "    np.bool8 = np.bool_\n",
        "\n",
        "import gym, random\n",
        "from collections import deque\n",
        "\n",
        "env = gym.make(\"FrozenLake-v1\", is_slippery=False)\n",
        "nS = env.observation_space.n\n",
        "nA = env.action_space.n\n",
        "\n",
        "α = 0.8\n",
        "γ = 0.95\n",
        "\n",
        "# ε parameters: start high, decay to min_epsilon\n",
        "ε_start, ε_min, ε_decay = 1.0, 0.01, 0.995\n",
        "\n",
        "n_episodes = 10000\n",
        "eval_interval = 1000\n",
        "\n",
        "Q = np.zeros((nS, nA))\n",
        "\n",
        "def unpack_reset(ret):\n",
        "    return ret if not isinstance(ret, tuple) else ret[0]\n",
        "\n",
        "def unpack_step(ret):\n",
        "    if len(ret) == 4:\n",
        "        s2, r, done, _ = ret\n",
        "    else:\n",
        "        s2, r, terminated, truncated, _ = ret\n",
        "        done = terminated or truncated\n",
        "    return s2, r, done\n",
        "\n",
        "def choose_action(s, ε):\n",
        "    return env.action_space.sample() if random.random() < ε else np.argmax(Q[s])\n",
        "\n",
        "# Track performance\n",
        "scores = []\n",
        "ε = ε_start\n",
        "for ep in range(1, n_episodes+1):\n",
        "    s = unpack_reset(env.reset())\n",
        "    done = False\n",
        "    while not done:\n",
        "        a = choose_action(s, ε)\n",
        "        s2, r, done = unpack_step(env.step(a))\n",
        "        Q[s,a] += α * (r + γ * np.max(Q[s2]) - Q[s,a])\n",
        "        s = s2\n",
        "    # decay ε\n",
        "    ε = max(ε_min, ε * ε_decay)\n",
        "\n",
        "    # periodically evaluate\n",
        "    if ep % eval_interval == 0:\n",
        "        # evaluate with ε=0\n",
        "        total = 0\n",
        "        for _ in range(100):\n",
        "            s = unpack_reset(env.reset())\n",
        "            done, ep_r = False, 0\n",
        "            while not done:\n",
        "                a = np.argmax(Q[s])\n",
        "                s, r, done = unpack_step(env.step(a))\n",
        "                ep_r += r\n",
        "            total += ep_r\n",
        "        avg = total / 100\n",
        "        scores.append((ep, avg))\n",
        "        print(f\"Episode {ep:5d}, ε={ε:.3f}, eval avg reward={avg:.2f}\")\n",
        "\n",
        "# final evaluation\n",
        "total = 0\n",
        "for _ in range(200):\n",
        "    s = unpack_reset(env.reset())\n",
        "    done, ep_r = False, 0\n",
        "    while not done:\n",
        "        a = np.argmax(Q[s])\n",
        "        s, r, done = unpack_step(env.step(a))\n",
        "        ep_r += r\n",
        "    total += ep_r\n",
        "print(\"Final average reward over 200 eval episodes:\", total/200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-u2K5PuZ3Dx"
      },
      "source": [
        "## 3. Policy Gradient (REINFORCE)\n",
        "\n",
        "* **Objective**: maksimalkan $\\mathbb{E}_\\pi[G_t]$\n",
        "* **Gradient estimator**:\n",
        "\n",
        "  $\n",
        "    \\nabla_\\theta J(\\theta)\n",
        "    \\approx \\frac{1}{N}\\sum_{i=1}^N \\sum_{t=0}^{T_i-1}\n",
        "      \\nabla_\\theta \\log\\pi_\\theta(a_t|s_t)\\;G_t\n",
        "  $\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWFzUnjHZPSQ",
        "outputId": "13bba6af-0a81-42f0-d85b-9cd3c770c564"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.11/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 0, loss=6.569\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 6 calls to <function train_step at 0x7cb6bdc66a20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 10 calls to <function train_step at 0x7cb6bdc66a20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 50, loss=11.049\n",
            "Episode 100, loss=9.164\n",
            "Episode 150, loss=21.965\n",
            "Episode 200, loss=23.967\n",
            "Episode 250, loss=6.766\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "n_inputs  = env.observation_space.shape[0]\n",
        "n_actions = env.action_space.n\n",
        "\n",
        "# Model policy\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Dense(32, activation=\"relu\", input_shape=(n_inputs,)),\n",
        "    keras.layers.Dense(n_actions, activation=\"softmax\")\n",
        "])\n",
        "optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
        "\n",
        "def run_episode(env, model):\n",
        "    states, actions, rewards = [], [], []\n",
        "    s, done = env.reset(), False\n",
        "    while not done:\n",
        "        probs = model(np.array([s], dtype=np.float32)).numpy()[0]\n",
        "        a = np.random.choice(n_actions, p=probs)\n",
        "        s2, r, done, _ = env.step(a)\n",
        "        states.append(s); actions.append(a); rewards.append(r)\n",
        "        s = s2\n",
        "\n",
        "    # compute returns\n",
        "    returns = []\n",
        "    G = 0\n",
        "    for r in reversed(rewards):\n",
        "        G = r + 0.99 * G\n",
        "        returns.insert(0, G)\n",
        "    returns = np.array(returns, dtype=np.float32)\n",
        "\n",
        "    return np.array(states, dtype=np.float32), \\\n",
        "           np.array(actions, dtype=np.int32), \\\n",
        "           returns\n",
        "\n",
        "@tf.function\n",
        "def train_step(states, actions, returns):\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = model(states, training=True)\n",
        "        act_masks = tf.one_hot(actions, n_actions)\n",
        "        log_probs = tf.reduce_sum(act_masks * tf.math.log(logits + 1e-8), axis=1)\n",
        "        loss = -tf.reduce_mean(log_probs * returns)\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "    return loss\n",
        "\n",
        "# Training loop\n",
        "for episode in range(300):\n",
        "    states, actions, returns = run_episode(env, model)\n",
        "    loss = train_step(states, actions, returns)\n",
        "    if episode % 50 == 0:\n",
        "        print(f\"Episode {episode}, loss={loss.numpy():.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x08GM28tZ-Wm"
      },
      "source": [
        "## 4. Deep Q‑Network (DQN)\n",
        "\n",
        "* Gunakan **NN** untuk approx $Q(s,a;\\theta)$\n",
        "* **Replay buffer**, **target network**, **ɛ‑greedy**\n",
        "* **Update**:\n",
        "\n",
        "  $\n",
        "    y = r + \\gamma \\max_{a'}Q(s',a';\\theta^-) ,\\quad\n",
        "    \\theta \\leftarrow \\arg\\min_\\theta (Q(s,a;\\theta)-y)^2\n",
        "  $"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_EOU329ZeiU",
        "outputId": "2bc8c9bd-d4ab-43c2-e04a-bd86ba9f8483"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep   0, reward 59.0, eps 0.995\n",
            "Ep  50, reward 18.0, eps 0.774\n",
            "Ep 100, reward 37.0, eps 0.603\n",
            "Ep 150, reward 14.0, eps 0.469\n",
            "Ep 200, reward 15.0, eps 0.365\n",
            "Ep 250, reward 72.0, eps 0.284\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "if not hasattr(np, \"bool8\"):\n",
        "    np.bool8 = np.bool_\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import gym, random\n",
        "from collections import deque\n",
        "\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "n_inputs  = env.observation_space.shape[0]\n",
        "n_actions = env.action_space.n\n",
        "\n",
        "def unpack_reset(ret):\n",
        "    return ret if not isinstance(ret, tuple) else ret[0]\n",
        "\n",
        "def unpack_step(ret):\n",
        "    if len(ret) == 4:\n",
        "        obs, reward, done, _ = ret\n",
        "    else:\n",
        "        obs, reward, terminated, truncated, _ = ret\n",
        "        done = terminated or truncated\n",
        "    return obs, reward, done\n",
        "\n",
        "# Build Q‑network\n",
        "def build_q_network():\n",
        "    model = keras.Sequential([\n",
        "        keras.layers.Dense(32, activation=\"relu\", input_shape=(n_inputs,)),\n",
        "        keras.layers.Dense(32, activation=\"relu\"),\n",
        "        keras.layers.Dense(n_actions)\n",
        "    ])\n",
        "    model.compile(optimizer=\"adam\", loss=\"mse\")\n",
        "    return model\n",
        "\n",
        "q_net = build_q_network()\n",
        "target_net = build_q_network()\n",
        "target_net.set_weights(q_net.get_weights())\n",
        "\n",
        "buffer = deque(maxlen=2000)\n",
        "epsilon = 1.0\n",
        "gamma = 0.99\n",
        "batch_size = 64\n",
        "update_target_every = 50\n",
        "\n",
        "episodes = 300\n",
        "for ep in range(episodes):\n",
        "    s = unpack_reset(env.reset())\n",
        "    done = False\n",
        "    ep_reward = 0\n",
        "    while not done:\n",
        "        if random.random() < epsilon:\n",
        "            a = env.action_space.sample()\n",
        "        else:\n",
        "            q_vals = q_net.predict(np.array([s], dtype=np.float32), verbose=0)[0]\n",
        "            a = np.argmax(q_vals)\n",
        "        s2, r, done = unpack_step(env.step(a))\n",
        "        buffer.append((s, a, r, s2, done))\n",
        "        s = s2\n",
        "        ep_reward += r\n",
        "\n",
        "        if len(buffer) >= batch_size:\n",
        "            batch = random.sample(buffer, batch_size)\n",
        "            states  = np.array([b[0] for b in batch], dtype=np.float32)\n",
        "            actions = np.array([b[1] for b in batch], dtype=np.int32)\n",
        "            rewards = np.array([b[2] for b in batch], dtype=np.float32)\n",
        "            next_s  = np.array([b[3] for b in batch], dtype=np.float32)\n",
        "            dones   = np.array([b[4] for b in batch], dtype=np.float32)\n",
        "\n",
        "            q_next = target_net.predict(next_s, verbose=0)\n",
        "            targets = rewards + gamma * np.max(q_next, axis=1) * (1 - dones)\n",
        "            q_vals  = q_net.predict(states, verbose=0)\n",
        "            q_vals[np.arange(batch_size), actions] = targets\n",
        "            q_net.train_on_batch(states, q_vals)\n",
        "\n",
        "    # Update target network\n",
        "    if ep % update_target_every == 0:\n",
        "        target_net.set_weights(q_net.get_weights())\n",
        "    epsilon = max(0.1, epsilon * 0.995)\n",
        "\n",
        "    if ep % 50 == 0:\n",
        "        print(f\"Ep {ep:3d}, reward {ep_reward:.1f}, eps {epsilon:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ev39rkBaYFu4"
      },
      "source": [
        "---\n",
        "\n",
        "## Ringkasan Chapter 18\n",
        "\n",
        "1. **Q‑Learning** tabular mempelajari Q‑table via Bellman update.\n",
        "2. **Policy Gradient (REINFORCE)** optimalkan langsung parameter policy.\n",
        "3. **DQN** gunakan neural network, replay buffer, dan target network untuk stabilitas.\n",
        "4. Banyak varian: Double DQN, Dueling DQN, Prioritized Experience Replay, Actor‑Critic, PPO, dll."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}